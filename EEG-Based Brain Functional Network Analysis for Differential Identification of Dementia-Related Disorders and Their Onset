"""
EEG-Based Depression Detection using CWT + iPLV Features with XGBoost
======================================================================
Based on dementia paper methodology but adapted for MDD vs H dataset

Key Features:
- CWT-based time-frequency analysis
- iPLV (instantaneous Phase Locking Value) for connectivity
- MSWC (Mean Squared Wavelet Coherence) for amplitude correlation
- XGBoost classifier for binary classification (MDD vs Healthy)

Dependencies:
- numpy, pandas, mne, scikit-learn, xgboost, pywt
"""

import os
import warnings
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import numpy as np
import pandas as pd
import mne
from pathlib import Path
import pywt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score
from sklearn.model_selection import StratifiedKFold
import xgboost as xgb
from datetime import datetime
from collections import defaultdict

mne.set_log_level('ERROR')

# ============================================================================
# OPTIMIZED CWT + iPLV FEATURE EXTRACTOR
# ============================================================================

class CWTiPLVExtractor:
    """
    Extract CWT+iPLV features following the paper's methodology:
    - CWT for time-frequency decomposition
    - iPLV for phase synchronization
    - MSWC for amplitude correlation
    - Feature fusion
    """
    
    def __init__(self, fs=256, window_sec=10, overlap=0.5):
        """
        Initialize extractor
        
        Args:
            fs: Sampling frequency (256 Hz as per paper)
            window_sec: Window size in seconds (10 sec as per paper)
            overlap: Overlap ratio (0.5 = 50%)
        """
        self.fs = fs
        self.window_sec = window_sec
        self.overlap = overlap
        self.window_samples = int(window_sec * fs)
        self.step_samples = int(self.window_samples * (1 - overlap))
        
        # Frequency bands (as per paper: delta, theta, alpha, beta, gamma)
        self.band_names = ['delta', 'theta', 'alpha', 'beta', 'gamma']
        self.bands = {
            'delta': (1.0, 4.0),
            'theta': (4.0, 8.0),
            'alpha': (8.0, 13.0),
            'beta': (13.0, 30.0),
            'gamma': (30.0, 45.0)
        }
        
        # Setup CWT parameters
        self._setup_cwt_scales()
    
    def _setup_cwt_scales(self):
        """Setup CWT scales to cover 1-45 Hz (as per paper)"""
        # Complex Morlet wavelet
        wavelet = 'cmor1.5-1.0'
        
        # Frequency range: 1-45 Hz
        min_freq = 1.0
        max_freq = 45.0
        n_freqs = 64
        
        # Logarithmically spaced frequencies
        freqs = np.logspace(np.log10(min_freq), np.log10(max_freq), n_freqs)
        
        # Convert to scales
        center_freq = 1.0
        self.scales = center_freq * self.fs / freqs
        self.frequencies = freqs
        self.wavelet = wavelet
        
        # Get band indices
        self.band_indices = {}
        for band_name in self.band_names:
            low, high = self.bands[band_name]
            indices = np.where((self.frequencies >= low) & (self.frequencies <= high))[0]
            self.band_indices[band_name] = indices
    
    def compute_cwt_for_channel(self, signal_data):
        """
        Compute CWT for entire channel
        
        Args:
            signal_data: 1D array of EEG signal
            
        Returns:
            cwt_matrix: Complex CWT coefficients (n_scales x n_samples)
        """
        try:
            cwt_coeffs, _ = pywt.cwt(
                signal_data,
                self.scales,
                self.wavelet,
                sampling_period=1.0/self.fs
            )
            return cwt_coeffs
        except Exception as e:
            print(f"  ⚠ CWT computation error: {str(e)[:50]}")
            return None
    
    def segment_cwt(self, cwt_matrix):
        """Segment CWT results into windows"""
        if cwt_matrix is None:
            return []
        
        segments = []
        n_time = cwt_matrix.shape[1]
        
        for start in range(0, n_time - self.window_samples + 1, self.step_samples):
            segment = cwt_matrix[:, start:start + self.window_samples]
            segments.append(segment)
        
        return segments
    
    def compute_iplv(self, cwt1_segment, cwt2_segment):
        """
        Compute instantaneous Phase Locking Value (iPLV)
        
        iPLV measures phase synchronization between two signals
        """
        # Extract instantaneous phase
        phase1 = np.angle(cwt1_segment)
        phase2 = np.angle(cwt2_segment)
        
        # Phase difference
        phase_diff = phase1 - phase2
        
        # iPLV: magnitude of mean complex phase difference
        mean_complex = np.mean(np.exp(1j * phase_diff), axis=1)
        iplv = np.abs(mean_complex)
        
        return iplv
    
    def compute_mswc(self, cwt1_segment, cwt2_segment):
        """
        Compute Mean Squared Wavelet Coherence (MSWC)
        
        MSWC measures amplitude correlation
        """
        # Cross-spectrum
        cross = cwt1_segment * np.conj(cwt2_segment)
        
        # Auto-spectra
        auto1 = np.abs(cwt1_segment) ** 2
        auto2 = np.abs(cwt2_segment) ** 2
        
        # Avoid division by zero
        denom = auto1 * auto2
        denom = np.where(denom < 1e-10, 1e-10, denom)
        
        # Squared wavelet coherence
        swc = np.clip(np.abs(cross) ** 2 / denom, 0, 1)
        
        # Mean over time
        mswc = np.mean(swc, axis=1)
        
        return mswc
    
    def fuse_iplv_mswc(self, iplv, mswc):
        """
        Feature fusion: non-linear combination of iPLV and MSWC
        """
        summed = np.clip(iplv, 0, 1) + np.clip(mswc, 0, 1)
        result = np.zeros_like(summed)
        
        # Region 1: sum <= 1
        mask1 = summed <= 1
        result[mask1] = (np.exp(summed[mask1]) - 1) / (2 * np.e - 2)
        
        # Region 2: sum > 1
        mask2 = summed > 1
        result[mask2] = 1 - (np.exp(2 - summed[mask2]) - 1) / (2 * np.e - 2)
        
        return result
    
    def extract_features_from_file(self, channel_data):
        """
        Extract all features from one file
        
        Process:
        1. Compute CWT once for each channel
        2. Segment the CWT results
        3. For each segment: compute connectivity features
        
        Returns:
            List of feature vectors (one per segment)
        """
        n_channels = len(channel_data)
        
        # Step 1: Compute CWT for all channels
        cwt_full = []
        for signal_data in channel_data:
            cwt_matrix = self.compute_cwt_for_channel(signal_data)
            if cwt_matrix is None:
                return []
            cwt_full.append(cwt_matrix)
        
        # Step 2: Segment all CWT results
        cwt_segments = []
        for cwt_matrix in cwt_full:
            segments = self.segment_cwt(cwt_matrix)
            cwt_segments.append(segments)
        
        if not cwt_segments or len(cwt_segments[0]) == 0:
            return []
        
        n_segments = len(cwt_segments[0])
        
        # Step 3: Extract features for each segment
        all_segment_features = []
        
        for seg_idx in range(n_segments):
            features = []
            
            # For each channel pair (functional connectivity)
            for i in range(n_channels):
                for j in range(i + 1, n_channels):
                    # Get CWT segment for this pair
                    cwt_i_seg = cwt_segments[i][seg_idx]
                    cwt_j_seg = cwt_segments[j][seg_idx]
                    
                    # Compute connectivity measures
                    iplv = self.compute_iplv(cwt_i_seg, cwt_j_seg)
                    mswc = self.compute_mswc(cwt_i_seg, cwt_j_seg)
                    
                    # Fuse iPLV and MSWC
                    p_mswc = self.fuse_iplv_mswc(iplv, mswc)
                    
                    # Extract per-band features
                    for band_name in self.band_names:
                        indices = self.band_indices[band_name]
                        if len(indices) > 0:
                            features.append(np.mean(p_mswc[indices]))
                        else:
                            features.append(0.0)
            
            all_segment_features.append(np.array(features))
        
        return all_segment_features


# ============================================================================
# EEG DATA LOADER (Following Paper's Preprocessing)
# ============================================================================

class EEGLoader:
    """
    Load and preprocess EEG data following the paper's methodology:
    - Bandpass filter: 1-45 Hz
    - Downsample to 256 Hz
    - Artifact removal
    - Use 23 standard electrodes
    """
    
    def __init__(self, fs=256):
        self.fs = fs
        # 23 electrodes as per paper
        self.channels = [
            'Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz', 'FT9', 'FT10',
            'T1', 'T2', 'T7', 'T8', 'C3', 'C4', 'Cz',
            'P3', 'P4', 'P7', 'P8', 'Pz', 'O1', 'O2'
        ]
    
    def load(self, filepath):
        """Load with preprocessing following paper"""
        try:
            raw = mne.io.read_raw_edf(filepath, preload=True, verbose=False)
            
            # Resample to 256 Hz
            if raw.info['sfreq'] != self.fs:
                raw.resample(self.fs, verbose=False)
            
            # Bandpass filter: 1-45 Hz (as per paper)
            raw.filter(1.0, 45.0, verbose=False)
            
            # Notch filter (50 Hz - power line noise)
            raw.notch_filter(50.0, verbose=False)
            
            return raw
        except Exception as e:
            print(f"  ⚠ Load error: {str(e)[:50]}")
            return None
    
    def extract_channels(self, raw):
        """Extract standard channels"""
        try:
            available = [ch.upper() for ch in raw.ch_names]
            data = []
            
            for target in self.channels:
                found = False
                for i, ch in enumerate(available):
                    if target.upper() in ch or ch in target.upper():
                        data.append(raw.get_data()[i])
                        found = True
                        break
                if not found:
                    # If channel not found, pad with zeros
                    data.append(np.zeros(raw.n_times))
            
            return np.array(data) if len(data) >= 10 else None
        except Exception as e:
            print(f"  ⚠ Channel extraction error: {str(e)[:50]}")
            return None


# ============================================================================
# DATASET MANAGEMENT - MODIFIED FOR MDD vs H
# ============================================================================

def load_dataset(data_path, condition='EC'):
    """
    Load dataset with labels
    
    Labels for Depression Dataset:
    - 0: Healthy Control (H)
    - 1: Major Depressive Disorder (MDD)
    """
    files = list(Path(data_path).glob(f'*{condition}.edf'))
    dataset = []
    
    for f in files:
        name = f.stem.upper()
        
        # Determine label - ONLY CHANGE FROM ORIGINAL
        if 'MDD' in name:
            label = 1  # Depression
        elif 'H ' in name or '_H ' in name or name.startswith('H ') or name.startswith('H_'):
            label = 0  # Healthy
        else:
            continue
        
        # Extract subject ID
        parts = name.replace('MDD', '').replace('H', '').replace('_', ' ').strip().split()
        subject_id = parts[0] if parts else name[:10]
        
        dataset.append({
            'file': f,
            'label': label,
            'subject': subject_id,
            'condition': condition
        })
    
    return dataset


def subject_wise_split(dataset, test_ratio=0.2, val_ratio=0.1, seed=42):
    """
    Subject-wise train-val-test split
    Ensures same subject doesn't appear in multiple sets
    """
    # Group by subject
    subjects = defaultdict(list)
    for item in dataset:
        subjects[item['subject']].append(item)
    
    # Group subjects by label
    label_subjects = defaultdict(list)
    for sid, items in subjects.items():
        label = items[0]['label']
        label_subjects[label].append(sid)
    
    # Split each label group
    np.random.seed(seed)
    train_data, val_data, test_data = [], [], []
    
    for label, sids in label_subjects.items():
        n = len(sids)
        n_test = max(1, int(n * test_ratio))
        n_val = max(1, int(n * val_ratio))
        n_train = n - n_test - n_val
        
        # Shuffle
        np.random.shuffle(sids)
        
        # Split
        train_sids = set(sids[:n_train])
        val_sids = set(sids[n_train:n_train+n_val])
        test_sids = set(sids[n_train+n_val:])
        
        # Collect items
        for sid, items in subjects.items():
            if sid in train_sids:
                train_data.extend(items)
            elif sid in val_sids:
                val_data.extend(items)
            elif sid in test_sids:
                test_data.extend(items)
    
    return train_data, val_data, test_data


# ============================================================================
# FEATURE EXTRACTION
# ============================================================================

def extract_features_from_dataset(dataset, loader, extractor, desc="Processing"):
    """Extract features with progress tracking"""
    X, y = [], []
    skipped = 0
    
    print(f"  {desc}: {len(dataset)} files...")
    start_time = datetime.now()
    last_timestamp = start_time
    
    for i, item in enumerate(dataset):
        current_time = datetime.now()
        
        # Progress every 20 seconds
        if (current_time - last_timestamp).total_seconds() >= 20:
            elapsed = (current_time - start_time).total_seconds()
            processed = i + 1
            rate = processed / (elapsed / 60) if elapsed > 0 else 0
            print(f"  [{processed}/{len(dataset)}] {elapsed:.0f}s | "
                  f"{rate:.1f} files/min | {item['file'].name[:30]}")
            last_timestamp = current_time
        
        # Load EEG
        raw = loader.load(item['file'])
        if raw is None:
            skipped += 1
            continue
        
        # Extract channels
        channel_data = loader.extract_channels(raw)
        if channel_data is None:
            skipped += 1
            continue
        
        # Extract features
        try:
            segment_features = extractor.extract_features_from_file(channel_data)
            
            # Add all segments
            for features in segment_features:
                X.append(features)
                y.append(item['label'])
        
        except Exception as e:
            print(f"  ⚠ Error in {item['file'].name}: {str(e)[:50]}")
            skipped += 1
            continue
    
    total_elapsed = (datetime.now() - start_time).total_seconds()
    avg_rate = len(dataset) / (total_elapsed / 60) if total_elapsed > 0 else 0
    print(f"  [{len(dataset)}/{len(dataset)}] {total_elapsed:.0f}s | "
          f"Avg: {avg_rate:.1f} files/min | Complete")
    
    if skipped > 0:
        print(f"  ⚠ Skipped {skipped} files")
    
    return np.array(X), np.array(y)


# ============================================================================
# XGBOOST CLASSIFIER
# ============================================================================

def train_xgboost(X_train, y_train, X_val, y_val, X_test, y_test):
    """
    Train XGBoost classifier for binary classification
    
    Returns:
        Dictionary with metrics and predictions
    """
    print("\n  Training XGBoost classifier...")
    
    # Standardize features
    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_val_s = scaler.transform(X_val)
    X_test_s = scaler.transform(X_test)
    
    # Number of classes
    n_classes = len(np.unique(y_train))
    
    # XGBoost parameters
    params = {
        'objective': 'multi:softmax' if n_classes > 2 else 'binary:logistic',
        'num_class': n_classes if n_classes > 2 else None,
        'max_depth': 6,
        'learning_rate': 0.1,
        'n_estimators': 200,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42,
        'verbosity': 0,
        'use_label_encoder': False,
        'eval_metric': 'mlogloss' if n_classes > 2 else 'logloss'
    }
    
    # Train
    clf = xgb.XGBClassifier(**params)
    clf.fit(X_train_s, y_train)
    
    # Predict
    y_pred = clf.predict(X_test_s)
    
    # Metrics
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    cm = confusion_matrix(y_test, y_pred)
    
    # Per-class metrics
    report = classification_report(y_test, y_pred, output_dict=True)
    
    return {
        'model': clf,
        'scaler': scaler,
        'accuracy': acc * 100,
        'f1_score': f1,
        'confusion_matrix': cm,
        'classification_report': report,
        'predictions': y_pred,
        'true_labels': y_test
    }


# ============================================================================
# RESULTS VISUALIZATION
# ============================================================================

def print_results(results, label_names):
    """Print classification results"""
    print(f"\n {'='*70}")
    print(f" XGBOOST CLASSIFICATION RESULTS")
    print(f" {'='*70}")
    print(f"  Overall Accuracy:  {results['accuracy']:.2f}%")
    print(f"  Weighted F1-Score: {results['f1_score']:.4f}")
    print(f"\n  Confusion Matrix:")
    
    cm = results['confusion_matrix']
    n_classes = len(cm)
    
    # Header
    print(f"  {'':12}", end='')
    for name in label_names[:n_classes]:
        print(f"{name:>8}", end='')
    print()
    
    # Rows
    for i, name in enumerate(label_names[:n_classes]):
        print(f"  {name:12}", end='')
        for j in range(n_classes):
            print(f"{cm[i,j]:8d}", end='')
        print()
    
    # Per-class metrics
    print(f"\n  Per-Class Metrics:")
    print(f"  {'Class':12} {'Precision':>10} {'Recall':>10} {'F1-Score':>10}")
    print(f"  {'-'*46}")
    
    report = results['classification_report']
    for i, name in enumerate(label_names[:n_classes]):
        if str(i) in report:
            metrics = report[str(i)]
            print(f"  {name:12} {metrics['precision']:10.4f} "
                  f"{metrics['recall']:10.4f} {metrics['f1-score']:10.4f}")
    
    print(f" {'='*70}\n")


# ============================================================================
# MAIN PIPELINE
# ============================================================================

def main():
    """Main pipeline"""
    start_time = datetime.now()
    
    print("\n" + "="*70)
    print(" EEG-BASED DEPRESSION DETECTION: CWT+iPLV + XGBOOST")
    print("="*70)
    print("  Methodology based on:")
    print("  'EEG-Based Brain Functional Network Analysis for")
    print("   Differential Identification of Dementia-Related Disorders'")
    print("  Modified to use CWT+iPLV features with XGBoost classifier")
    print("  Dataset: MDD (Depression) vs H (Healthy)")
    print("="*70)
    print(f"  Start: {start_time.strftime('%H:%M:%S')}\n")
    
    # Check dependencies
    try:
        import pywt
    except ImportError:
        print("\n✗ Error: PyWavelets not installed")
        print("  Install with: pip install PyWavelets")
        return
    
    # Data path
    DATA_PATH = '/kaggle/input/eeg-dataset/'
    
    if not Path(DATA_PATH).exists():
        print(f"✗ Error: {DATA_PATH} not found")
        print("\nUsing current directory instead...")
        DATA_PATH = '.'
    
    # Label names - CHANGED FOR DEPRESSION
    label_names = ['Healthy', 'MDD']
    
    # Step 1: Load dataset
    print("STEP 1: Load dataset")
    t1 = datetime.now()
    dataset = load_dataset(DATA_PATH, 'EC')
    
    if len(dataset) == 0:
        print("  ✗ No data files found")
        print("  Please ensure EEG files are in the correct format:")
        print("    - Files should contain 'MDD' or 'H' in filename")
        print("    - Files should end with 'EC.edf'")
        return
    
    print(f"  ✓ {len(dataset)} files found ({(datetime.now()-t1).total_seconds():.1f}s)")
    
    # Count per class
    label_counts = defaultdict(int)
    for item in dataset:
        label_counts[item['label']] += 1
    
    print("  Distribution:")
    for label, count in sorted(label_counts.items()):
        print(f"    {label_names[label]}: {count} files")
    print()
    
    # Step 2: Train-val-test split
    print("STEP 2: Train-validation-test split (subject-wise)")
    t2 = datetime.now()
    train_data, val_data, test_data = subject_wise_split(dataset)
    print(f"  ✓ Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)} "
          f"({(datetime.now()-t2).total_seconds():.1f}s)\n")
    
    # Step 3: Extract features
    print("STEP 3: Extract CWT+iPLV features\n")
    t3 = datetime.now()
    
    loader = EEGLoader()
    extractor = CWTiPLVExtractor()
    
    print("  Training set:")
    t3a = datetime.now()
    X_train, y_train = extract_features_from_dataset(train_data, loader, extractor, "Training")
    t3a_time = (datetime.now()-t3a).total_seconds()
    
    print("\n  Validation set:")
    t3b = datetime.now()
    X_val, y_val = extract_features_from_dataset(val_data, loader, extractor, "Validation")
    t3b_time = (datetime.now()-t3b).total_seconds()
    
    print("\n  Test set:")
    t3c = datetime.now()
    X_test, y_test = extract_features_from_dataset(test_data, loader, extractor, "Test")
    t3c_time = (datetime.now()-t3c).total_seconds()
    
    # Check if we got valid data
    if len(X_train) == 0 or len(X_test) == 0:
        print("\n  ✗ Error: No features extracted!")
        print("    Check that EEG files are valid and preprocessing is working")
        return
    
    print(f"\n  ✓ Feature extraction complete")
    print(f"    Features per segment: {X_train.shape[1]}")
    print(f"    Train: {len(X_train)} segments ({t3a_time:.1f}s)")
    print(f"    Val:   {len(X_val)} segments ({t3b_time:.1f}s)")
    print(f"    Test:  {len(X_test)} segments ({t3c_time:.1f}s)")
    
    # Step 4: Train XGBoost
    print("\nSTEP 4: Train XGBoost classifier")
    t4 = datetime.now()
    results = train_xgboost(X_train, y_train, X_val, y_val, X_test, y_test)
    t4_time = (datetime.now()-t4).total_seconds()
    print(f"  ✓ Training complete ({t4_time:.1f}s)")
    
    # Step 5: Print results
    print_results(results, label_names)
    
    # Summary
    total = (datetime.now()-start_time).total_seconds()
    
    print("="*70)
    print(" SUMMARY")
    print("="*70)
    print(f"  Total Execution Time: {total:.1f}s ({total/60:.2f} min)")
    print(f"  Final Accuracy: {results['accuracy']:.2f}%")
    print(f"  Final F1-Score: {results['f1_score']:.4f}")
    print(f"  End: {datetime.now().strftime('%H:%M:%S')}")
    print("="*70 + "\n")
    
    # Save results
    output_path = '/mnt/user-data/outputs/cwt_iplv_depression_results.csv'
    
    # Create summary dataframe
    summary_data = {
        'Metric': ['Accuracy', 'F1-Score', 'Training Time'],
        'Value': [
            f"{results['accuracy']:.2f}%",
            f"{results['f1_score']:.4f}",
            f"{total:.1f}s"
        ]
    }
    df_summary = pd.DataFrame(summary_data)
    df_summary.to_csv(output_path, index=False)
    
    print(f"  Results saved to: {output_path}\n")
    
    return results


if __name__ == "__main__":
    main()
